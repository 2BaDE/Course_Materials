{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "564bfa23",
   "metadata": {},
   "source": [
    "# Functional Programming\n",
    "\n",
    "## What is Functional Programming\n",
    "\n",
    "**함수형 프로그래밍(Functional Programming)**은 자료 처리를 수학적 함수의 계산으로 취급하고 상태와 가변 데이터를 멀리하는 프로그래밍 패러다임 중 하나이다. 명령형 프로그래밍에서는 상태를 바꾸는 것을 강조하는 것과는 달리, 함수형 프로그래밍은 함수의 응용을 강조한다. \n",
    "\n",
    "예를 들어, LineCounter 클래스는 파일을 읽고, 각각의 행을 읽어, 총 행의 수를 계산하는 클래스이다. 객체 지향형(Object Oriented) 관점에서는 속성과 메소드의 개념을 사용하여 해석한다. 속성은 객체의 상태를 의미하고, 메소드는 객체의 상태를 변화하는 것이다. 이때 객체의 상태는 메소드를 사용할 때마다 변하게 된다. 객체의 변화하는 상태는 좋을 수도 있고 나쁠 수도 있다. \n",
    "\n",
    "대표적으로 웹서버의 로그파일을 불러와서 로그의 수를 세는 함수를 작성하는 것을 예로 들 수 있다. \n",
    "\n",
    "```Python\n",
    "def read(filename) : \n",
    "    file = open(filename, 'r') \n",
    "    lines = [line for line in file]\n",
    "    return lines \n",
    "\n",
    "def count(lst) : \n",
    "    return len(lst)\n",
    "\n",
    "example_lines = read('example_log.txt')\n",
    "lines_count = count(example_lines)\n",
    "```\n",
    "\n",
    "## Pure Function\n",
    "\n",
    "함수만을 사용해서 기능을 구현하는 것을 **함수형 프로그래밍(Functional Programming)**이라고 한다. 함수형 프로그램 내부에서는 **상태가 없으며(stateless)**, 주어진 input과 생성된 output에만 의존하게 된다. \n",
    "\n",
    "함수형 프로그래밍을 위한 기준을 만족하는 함수를 **순수한 함수(Pure Function)**이라고 한다. 순수한 함수란 부작용이 없는 함수, 즉 함수의 실행이 외부에 영향을 끼치지 않는 함수를 뜻한다. 순수한 함수를 사용하는 장점은 부작용의 감소를 창출할 수있다. 부작요은 함수가 작동하는 내부에서 범위를 넘어서는 변화가 일어났을 때 발생하게 된다. 예를 들어 객체의 상태를 변화했을 때나, I/O 연산을 했을 때나, 심지어 print() 함수를 사용할 때도 발생하게 된다. \n",
    "\n",
    "프로그래머는 코드 내부의 부작용을 줄여 테스트하기 쉽고, 디버깅하기 쉬운 코드를 만들어야 한다. 코드에 부작용이 많아질수록, 프로그램이나 실행을 이해하기 어려워진다. \n",
    "\n",
    "## The Lambda Expression\n",
    "\n",
    "함수를 선언하기 위해 def 문법을 사용하는 방법 대신에 **lambda expression**을 사용해서 함수를 선언하게 된다. Lambda expression은 comma로 분리된 연속적인 입력값을 받고, colon(:)뒤에 있는 표현을 return statement없이 즉시 반환하게 된다. 생성한 lambda expression을 변수에 할당하게 되면 실제 함수처럼 작동하게 된다. \n",
    "\n",
    "변수명 없이 람다 표현을 사용하게 되면 **익명 함수(anonymous function)**이라고 부르게 된다. 익명함수는 다른 함수의 입력값으로 사용할 때 유용하게 사용된다. \n",
    "\n",
    "```Python\n",
    "def read(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return [line for line in f]\n",
    "    \n",
    "lines = read('example_log.txt')\n",
    "sorted_lines = sorted(lines, key = lambda x : x.split(' ')[5])\n",
    "print(sorted_lines)\n",
    "```\n",
    "\n",
    "## Higher-Order Function\n",
    "\n",
    "### The Map Function\n",
    "\n",
    "**고계 함수(higher-order function)**란, 함수를 다루는 함수를 뜻한다. 사실 함수형 언어에서는 함수도 '값(value)'으로 취급한다. 그러므로 정수 1이나 인수를 제곱하는 함수나 동등한 입장에서 다룰 수 있게 된다. 정수를 함수의 인수로 전달할 수 있듯이 어떠한 함수도 다른 함수의 인수로 전달할 수 있다. 즉, 앞서 사용했던 sorted 의 argument에 익명함수를 사용한 방식과 동일하다. \n",
    "\n",
    "함수형 패러다임에서 흔하게 사용되는 중요한 고계함수들은 다음과 같다. sorted 함수와 같이 리스트 내부의 각각의 원소에 대해 함수를 적용하는 Python iterable를 가지고 있는 함수들이다. 일반적으로 function_name(function_to_apply, iterable_of_elements)의 형태를 가지고 있다. \n",
    "\n",
    "map 함수는 첫번째 인수로 주어진 함수를 두번째 인수로 주어진 각 원소에 적용한 결과를 Map 객체로 반환한다. \n",
    "\n",
    "```Python\n",
    "# Pseudocode for map.\n",
    "def map(func, seq):\n",
    "    # Return `Map` object with\n",
    "    # the function applied to every\n",
    "    # element.\n",
    "    return Map(\n",
    "        func(x)\n",
    "        for x in seq\n",
    "    )\n",
    "\n",
    "# Map each line in the lines variable to its corresponding IP address\n",
    "lines = read('example_log.txt')\n",
    "ip_addresses = list(map(lambda x : x.split(' ')[0], lines))\n",
    "print(ip_addresses)\n",
    "```\n",
    "\n",
    "### The Filter Function\n",
    "\n",
    "filter 함수도 map 함수와 동일하게 반복되는 값을 받은 후에 조건문에 의해 생성된 bool값을 가지는 고계함수 이다. \n",
    "\n",
    "```Python\n",
    "# Pseudocode for filter.\n",
    "def filter(evaluate, seq):\n",
    "    # Return `Map` object with\n",
    "    # the evaluate function applied to every\n",
    "    # element.\n",
    "    return Map(\n",
    "        x for x in seq\n",
    "        if evaluate(x) is True\n",
    "    )\n",
    "\n",
    "# Filter each line in ip_addresses list to IP addreses that begin with less than or equal to 20.\n",
    "lines = read('example_log.txt')\n",
    "ip_addresses = list(map(lambda x: x.split()[0], lines))\n",
    "filtered_ips = list(filter(lambda x: int(x.split('.')[0]) <= 20, ip_addresses))\n",
    "print(filtered_ips)\n",
    "```\n",
    "\n",
    "### The Reduce Function\n",
    "\n",
    "functools package의 reduce 함수는 반복되는 값을 받아서 리스트 내부의 값들을 주어진 함수를 적용하여 하나의 값으로 축약하는 함수이다. 함수에 대해 첫번째 원소와 두번째 원소를 사용해 결과를 출력하고 다음 원소와 동일한 연산을 반복하게 된다. Lambda expression 내부에서 두번째 값에 대해 연산을 할 필요가 없는 경우 \\_를 사용해 무시할 수 있다. \n",
    "\n",
    "reduce 함수의 중요한 점은 input으로 동일한 타입의 값을 리턴할 필요가 없다는 뜻이다. 예를들어 리스트 내부의 모든 단어의 길이를 계산하는 lambda expression을 생성한다면 두번째 연산에 대해서 TypeError : object of type 'int' has no len()을 출력할 것이다. 이 문제를 해결하기 위해 lambda expression을 if-else 문을 활용해서 수정할 수 있다. \n",
    "\n",
    "```Python\n",
    "from functools import reduce\n",
    "\n",
    "lines = read('example_log.txt')\n",
    "ip_addresses = list(map(lambda x: x.split()[0], lines))\n",
    "filtered_ips = list(filter(lambda x: int(x.split('.')[0]) <= 20, ip_addresses))\n",
    "num_lines = reduce(lambda x, _: 2 if isinstance(x, str) else x + 1, lines)\n",
    "num_filtered = reduce(lambda x, _: 2 if isinstance(x, str) else x + 1, filtered_ips) \n",
    "ratio = num_filtered / num_lines\n",
    "print(ratio)\n",
    "```\n",
    "\n",
    "### Rewriting with List Comprehension \n",
    "\n",
    "map, filter 함수는 결과적으로 list로 변환해야하기 때문에 list comprehension을 사용해서 코드를 다시 작성할 수 있다. \n",
    "\n",
    "```Python\n",
    "lines = read('example_log.txt')\n",
    "# Rewrite ip_addresses, and filtered_ips.\n",
    "# list(map(lambda x: x.split()[0], lines))\n",
    "ip_addresses = [x.split()[0] for x in lines]\n",
    "# list(filter(lambda x: int(x.split('.')[0]) <= 20, ip_addresses))\n",
    "filtered_ips = [x for x in ip_addresses if int(x.split('.')[0]) <= 20]\n",
    "count_all = reduce(lambda x, _: 2 if isinstance(x, str) else x + 1, lines)\n",
    "count_filtered = reduce(lambda x, _: 2 if isinstance(x, str) else x + 1, filtered_ips)\n",
    "ratio = count_filtered / count_all\n",
    "print(ratio)\n",
    "```\n",
    "\n",
    "## Writing Function Partials \n",
    "\n",
    "함수의 기능은 유지하면서 함수의 입력값을 감소하고자 하는 경우, 하나의 입력값을 저장하고 있는 새로운 함수를 생성하면 된다. functools package의 partial modeule은 함수의 default값을 입력받아 그 상태의 함수를 '얼리는'것 과 같은 효과를 가진 함수를 새로 생성한다. 생성된 함수는 입력되지 않은 남은 입력값을 입력받아 기존의 함수의 기능을 실행하게 된다. \n",
    "\n",
    "```Python\n",
    "from functools import partial\n",
    "\n",
    "lines = read('example_log.txt')\n",
    "ip_addresses = list(map(lambda x: x.split()[0], lines))\n",
    "filtered_ips = list(filter(lambda x: int(x.split('.')[0]) <= 20, ip_addresses))\n",
    "\n",
    "# reduce(lambda x, _: 2 if isinstance(x, str) else x + 1, lines)\n",
    "extract = partial(reduce, lambda x, _: 2 if isinstance(x, str) else x + 1)\n",
    "count_all = extract(lines) \n",
    "# reduce(lambda x, _: 2 if isinstance(x, str) else x + 1, filtered_ips)\n",
    "count_filtered = extract(filtered_ips)\n",
    "\n",
    "ratio = count_filtered / count_all\n",
    "print(ratio)\n",
    "```\n",
    "\n",
    "## Using Functional Composition \n",
    "\n",
    "함수 호출은 연속적으로 시행하여 한 함수의 결과값을 다른 함수의 입력값으로 연결하는 작업을 수학적으로 **합성 함수(Function Composition)**이라고 한다. compose 함수는 single argument 함수를 연속으로 입력받아 하나로 연결된 함수를 생성한다. 파일 시스템의 pipeline 연산자 '|'와 동일하다. \n",
    "\n",
    "```Python\n",
    "lines = read('example_log.txt')\n",
    "ip_addreses = list(map(lambda x: x.split()[0], lines)) \n",
    "filtered_ips = list(filter(lambda x: int(x.split('.')[0]) <= 20, ip_addresses)) \n",
    "\n",
    "ratio = count_flitered / count_all \n",
    "extract_ips = partial(map, lambda x: x.split()[0])\n",
    "extract_filtered = partial(filter, lambda x: int(x.split('.')[0]) <= 20) \n",
    "count = partial(reduce, lambda x, _: 2 if isinstance(x, str) else x + 1)\n",
    "\n",
    "composed = compose(extract_ips, \n",
    "                   extract_filtered, \n",
    "                   count)\n",
    "counted = composed(lines) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7e839",
   "metadata": {},
   "source": [
    "# Pipline Tasks\n",
    "\n",
    "## What is Pipeline \n",
    "\n",
    "컴퓨터 과학에서 **파이프라인(Pipeline)**은 데이터 처리 단계의 출력이 다음 단계의 입력으로 이어지는 형태로 연결된 구조를 가리킨다. 이렇게 연결된 처리는 한 여러 단계가 서로 동시에 또는 병렬적으로 수행될 수 있어 효율성의 향상을 꾀할 수 있다. \n",
    "\n",
    "강의의 파이프라인의 목표는 이전에 사용했던 example_log.txt 파일에서 받은 log 데이터를 고유한 HTTP request types에 따른 CSV파일로 저장하고, 타입에 따른 수를 세는 것이다. 즉 원본데이터를 '추출'해서 '변환'한 뒤에 '저장'하여 '처리'하는 데이터 흐름이다. \n",
    "\n",
    "전송된 로그 파일은 다음의 형식을 따르고 있다. \n",
    "\n",
    "```Bash\n",
    "$remote_addr - $remote_user [$time_local]  \"$request\" $status $body_bytes_sent \"$http_referer\" \"$http_user_age\"\n",
    "```\n",
    "\n",
    "- \\$remote_addr : the ip address of the client making the request to the server\n",
    "- \\$remote_user : if the client authenticated with basic authentication, this is the user name (blank in the exmaple above).\n",
    "- \\$time_local : the local time when the request was made.\n",
    "- \\$request : the type of request, and the URL that it was made to\n",
    "- \\$status : the response status code from teh surver\n",
    "- \\$body_bytes_sent : the number of bytes sent by the server to the client in the response body\n",
    "- \\$http_refferrer : the page that the client was on before sending the current request\n",
    "- \\$http_user_agen : information about the browser and system of the client\n",
    "\n",
    "\n",
    "## Generators in Python\n",
    "\n",
    "example_log.txt 파일 내부의 데이터를 리스트로 읽어올 때, 파이썬은 리스트의 요소를 RAM 내부에 적재한다. 만일 파일이 수 기가바이트를 초과하게 되면, 파일 로딩은 메모리가 부족해 작업을 수행할 수 없게 된다. 따라서 파일을 메모리로 읽어보이는 방법 대신에 **파일 스트리밍(File Streaming)**을 사용해서 문제를 해결할 수 있다. 파일 스트리밍은 파일을 **청크(Chunk)**라고 불리는 작은 단위로 분해하여 메모리에 적재하는 것이다.\n",
    "\n",
    "하나의 청크가 처리되면 파이썬은 다음 청크를 요청하게 되고, 다음 청크가 반복해서 메모리에 적재되게 된다. 이같은 방식은 대용량 데이터셋을 처리하는데 효과적이다. 이를 구현하기 위해선 **제너레이터(generators)를 사용해서 iterable한 객체를 생성할 수 있다. \n",
    "\n",
    "제너레이터는 함수와는 두가지 면에서 다르다.\n",
    "\n",
    "- 제너레이터는 return 대신에 yield를 사용한다.(return의 경우 iteration을 종료시킬때만 사용한다.)\n",
    "- 지역변수는 제너레이터가 완료될때까지 메모리 내부에 남아있는다.\n",
    "\n",
    "생성된 제너레이터는 두가지 행동을 책임지게 된다.\n",
    "\n",
    "- Python 인터프리터에게 현재 함수가 제너레이터임을 알린다.\n",
    "- 함수의 실행을 유예하여 메모리 내부에 지역변수를 저장하게 된다.\n",
    "\n",
    "next() 함수는 제너레이터가 유예시킨 작업을 확인할 수 있다. for loop의 반복문과 마찬가지로 제너레이터에 의해 생성된 iterable한 객체를 next 함수를 통해 하나씩 확인할 수 있다. 제너레이터의 실행을 종료하고 싶은 경우, 제너레이터를 종료하고자 하는 조건에서 return statement를 선언하면 된다.\n",
    "\n",
    "```Python\n",
    "def squares(N) : \n",
    "    i = 0\n",
    "    while True : \n",
    "        if i == 20 : \n",
    "            return \n",
    "        yield i * i\n",
    "        i += 1 \n",
    "        \n",
    "squared_values = list(squares(20))\n",
    "```\n",
    "\n",
    "## Generator Comprehension\n",
    "\n",
    "**Generator Comprehension은 List Comprehension과 매우 유사하다. 생성된 List Comprehension은 [ ] 대신에 ( )로 대체함으로써 generator object를 생성할 수 있다. 하지만 list comprehension과 달리 generator comprehension은 yield된 요소가 한번 실행되면 메모리에서 전부 소모되어 버린다. \n",
    "\n",
    "```Python\n",
    "squared_list = [i * i for i in range(20)]\n",
    "squared_gen = (i * i for i in range(20))\n",
    "```\n",
    "\n",
    "## Generators in Tasks(Extract and Transfer) \n",
    "\n",
    "### Extract\n",
    "\n",
    "Law rog file을 요약된 CSV로 처리하는 워크플로우는 다음과 같다.\n",
    "\n",
    "1. example_log.txt 파일을 읽는다.\n",
    "2. 로그 파일을 여러 행으로 파싱한다.\n",
    "3. 행들을 CSV파일로 포맷한다.\n",
    "4. CSV파일에서 분석을 수행한 뒤에, unique visitors의 수를 센다.\n",
    "5. 분석 결과를 CSV파일로 요약한다.\n",
    "\n",
    "각 단계는 개별적인 작업으로 고정될 수 있다. 또한 iterable한 객체 형태로 입력되어야 하고 iterable한 객체 형태로 출력되어야 한다. 또한 함수형 프로그래밍을 위한 일반적인 원칙(순수한 함수의 정의를 만족하면서 합성함수를 구성)을 고수해야한다.\n",
    "\n",
    "parse_log는 로그 파일의 데이터를 읽어 각 레코드 별로 데이터를 추출하여 immutable한 iterable 객체인 tuple에 저장한다.\n",
    "\n",
    "```Python\n",
    "log = open('example_log.txt')\n",
    "\n",
    "def parse_log(log): \n",
    "    for line in log: \n",
    "        split_line = line.split() \n",
    "        remote_addr = split_line[0]\n",
    "        time_local = split_line[3] + \" \" + split_line[4]\n",
    "        request_type = split_line[5]\n",
    "        request_path = split_line[6]\n",
    "        status = split_line[8]\n",
    "        body_bytes_sent = split_line[9]\n",
    "        http_referrer = split_line[10]\n",
    "        http_user_agent = \" \".join(split_line[11:])\n",
    "        yield (remote_addr, time_local, request_type, request_path, status, body_bytes_sent, http_referrer, http_user_agent)\n",
    "        \n",
    "first_line = next(parse_log(log))\n",
    "```\n",
    "\n",
    "### Transfer \n",
    "\n",
    "제너레이터를 사용해서 iterable한 객체를 생성하기 전에 레코드에 따른 데이터 전처리 또한 진행한다.\n",
    "\n",
    "```Python\n",
    "log = open('example_log.txt')\n",
    "\n",
    "def parse_time(time_str):\n",
    "    \"\"\"\n",
    "    Parses time in the format [30/Nov/2017:11:59:54 +0000]\n",
    "    to a datetime object.\n",
    "    \"\"\"\n",
    "    time_obj = datetime.strptime(time_str, '[%d/%b/%Y:%H:%M:%S %z]')\n",
    "    return time_obj\n",
    "\n",
    "def strip_quotes(s):\n",
    "    return s.replace('\"', '')\n",
    "\n",
    "def parse_log(log):\n",
    "    for line in log:\n",
    "        split_line = line.split()\n",
    "        remote_addr = split_line[0]\n",
    "        time_local = parse_time(split_line[3] + \" \" + split_line[4])\n",
    "        request_type = strip_quotes(split_line[5])\n",
    "        request_path = split_line[6]\n",
    "        status = int(split_line[8])\n",
    "        body_bytes_sent = int(split_line[9])\n",
    "        http_referrer = strip_quotes(split_line[10])\n",
    "        http_user_agent = strip_quotes(\" \".join(split_line[11:]))\n",
    "        yield (\n",
    "            remote_addr, time_local, request_type, request_path,\n",
    "            status, body_bytes_sent, http_referrer, http_user_agent\n",
    "        )\n",
    "        \n",
    "first_line = next(parse_log(log))\n",
    "```\n",
    "\n",
    "## Write to CSV(Load) \n",
    "\n",
    "### Load on List \n",
    "\n",
    "build_csv 함수는 입력받은 iterable 객체를 CSV파일로 포맷하여 저장한다.\n",
    "\n",
    "```Python\n",
    "import csv\n",
    "\n",
    "log = open('example_log.txt')\n",
    "parsed = parse_log(log)\n",
    "\n",
    "def build_csv(lines, file, header = None): \n",
    "    if header : \n",
    "        lines = [header] + [line for line in lines] \n",
    "    writer = csv.writer(file, delimiter = ',')\n",
    "    writer.writerows(lines) \n",
    "    file.seek(0)\n",
    "    return file\n",
    "\n",
    "file = open('temporary.csv', 'r+')\n",
    "csv_file = build_csv(parsed, file, ['ip', 'time_local', 'request_type', 'request_path', 'status', 'bytes_sent', 'http_referrer', 'http_user_agent'])\n",
    "contents = csv_file.readlines() \n",
    "print(contents[:5])\n",
    "```\n",
    "\n",
    "### Load within generator \n",
    "\n",
    "build_csv 함수에서 header를 추가하기 위해 제너레이터로부터 생성된 행을 리스트로 변환했어야 했다. 객체를 변환함으로써 행을 스트리밍하는 장점을 잃게되어 제너레이터를 생성한 의미가 없어진다. 제너레이터 행동을 유지하기위해 itertools.chain() 메소드를 사용해서 두 iterable를 하나로 포함시킬 수 있다.(일반적인 연산으로 진행하게 되면 메모리 내부에 적재되어있는 지역변수를 모두 소모해서 다음 연산을 진행할 수 없다.)\n",
    "\n",
    "```Python\n",
    "import csv\n",
    "import itertools \n",
    "\n",
    "log = open('example_log.txt')\n",
    "parsed = parse_log(log)\n",
    "\n",
    "def build_csv(lines, file, header=None):\n",
    "    # if header:\n",
    "    #    lines = [header] + [l for l in lines]\n",
    "    if header:\n",
    "        lines = itertools.chain([header], lines)\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerows(lines)\n",
    "    file.seek(0)\n",
    "    return file\n",
    "\n",
    "file = open('temporary.csv', 'r+')\n",
    "csv_file = build_csv(\n",
    "    parsed,\n",
    "    file,\n",
    "    header=[\n",
    "        'ip', 'time_local', 'request_type',\n",
    "        'request_path', 'status', 'bytes_sent',\n",
    "        'http_referrer', 'http_user_agent'\n",
    "    ]\n",
    ")\n",
    "    \n",
    "contents = csv_file.readlines()\n",
    "print(contents[:5])\n",
    "```\n",
    "\n",
    "## Counting Unique Request Type \n",
    "\n",
    "count_unique_request는 생성된 csv_file을 받아서 request type별 빈도수를 계산하여 dictionary를 리턴한다.\n",
    "\n",
    "```Python\n",
    "import csv\n",
    "\n",
    "log = open('example_log.txt')\n",
    "parsed = parse_log(log)\n",
    "file = open('temporary.csv', 'r+')\n",
    "csv_file = build_csv(\n",
    "    parsed,\n",
    "    file,\n",
    "    header=[\n",
    "        'ip', 'time_local', 'request_type',\n",
    "        'request_path', 'status', 'bytes_sent',\n",
    "        'http_referrer', 'http_user_agent'\n",
    "    ]\n",
    ")\n",
    "\n",
    "def count_unique_request(csv_file) : \n",
    "    reader = csv.reader(csv_file) \n",
    "    header = next(reader) \n",
    "    index = header.index('request_type') \n",
    "    \n",
    "    uniques = {} \n",
    "    for row in reader : \n",
    "        request_type = row[index]\n",
    "        if request_type in uniques : \n",
    "            uniques[request_type] += 1\n",
    "        else : \n",
    "            uniques[request_type] = 1\n",
    "            \n",
    "    return uniques \n",
    "\n",
    "uniques = count_unique_request(csv_file) \n",
    "print(uniques)\n",
    "```\n",
    "\n",
    "## Full Process \n",
    "\n",
    "마지막으로 생성된 uniques를 build_csv 함수를 사용해서 CSV format으로 저장한다.  build_csv는 generator를 실행하기 위해 iterable 객체를 입력받는다. 따라서 count_unique_requests 함수는 ( )를 통해 값을 리턴해야 한다.\n",
    "\n",
    "```Python\n",
    "# Libraries for Pipeline \n",
    "import csv \n",
    "import itertools \n",
    "\n",
    "# Functions for Pipeline \n",
    "\n",
    "def parse_time(time_str):\n",
    "    \"\"\"\n",
    "    Parses time in the format [30/Nov/2017:11:59:54 +0000]\n",
    "    to a datetime object.\n",
    "    \"\"\"\n",
    "    time_obj = datetime.strptime(time_str, '[%d/%b/%Y:%H:%M:%S %z]')\n",
    "    return time_obj\n",
    "\n",
    "def strip_quotes(s):\n",
    "    return s.replace('\"', '')\n",
    "\n",
    "def parse_log(log):\n",
    "    for line in log:\n",
    "        split_line = line.split()\n",
    "        remote_addr = split_line[0]\n",
    "        time_local = parse_time(split_line[3] + \" \" + split_line[4])\n",
    "        request_type = strip_quotes(split_line[5])\n",
    "        request_path = split_line[6]\n",
    "        status = int(split_line[8])\n",
    "        body_bytes_sent = int(split_line[9])\n",
    "        http_referrer = strip_quotes(split_line[10])\n",
    "        http_user_agent = strip_quotes(\" \".join(split_line[11:]))\n",
    "        yield (\n",
    "            remote_addr, time_local, request_type, request_path,\n",
    "            status, body_bytes_sent, http_referrer, http_user_agent\n",
    "        )\n",
    "        \n",
    "def build_csv(lines, file, header=None):\n",
    "    # if header:\n",
    "    #    lines = [header] + [l for l in lines]\n",
    "    if header:\n",
    "        lines = itertools.chain([header], lines)\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerows(lines)\n",
    "    file.seek(0)\n",
    "    return file      \n",
    "\n",
    "def count_unique_request(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('request_type')\n",
    "    \n",
    "    uniques = {}\n",
    "    for line in reader:\n",
    "        \n",
    "        if not uniques.get(line[idx]):\n",
    "            uniques[line[idx]] = 0\n",
    "        uniques[line[idx]] += 1\n",
    "    return ((k, v) for k, v in uniques.items())\n",
    "\n",
    "# Execute Pipeline \n",
    "\n",
    "log = open('example_log.txt')\n",
    "parsed = parse_log(log)\n",
    "file = open('temporary.csv', 'r+')\n",
    "csv_file = build_csv(parsed,file, header=['ip', 'time_local', 'request_type', 'request_path', 'status', 'bytes_sent', 'http_referrer', 'http_user_agent'])\n",
    "uniques = count_unique_request(csv_file)\n",
    "summarized_file = open('summarized.csv', 'r+')\n",
    "summarized_csv = build_csv(uniques, summarized_file, ['request_type', 'count'])\n",
    "print(summarized_file.readlines())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ebeef0",
   "metadata": {},
   "source": [
    "# Building a Pipeline Class \n",
    "\n",
    "## General Purpose Pipeline \n",
    "\n",
    "이전에 구성했던 일련의 작업들은 코드들이 **정적(statically)**으로 구성되었다는 것이다. 즉, 작업들은 특정한 목적을 위해 작성되었으며 이에 따라 함수 호출을 위한 적절한 변수들을 선언해야 했다. 이 과정은 새로운 작업이나 프로세스를 추가할 때 파이프라인을 확장하는데에 있어 어려움을 야기하게 된다. \n",
    "\n",
    "따라서 일반적인 목적을 가지고 있는 파이프라인을 설계하는 것은 작업을 쉽게 만들고 확장하는데 도움을 줄 수 있다. 함수를 호출하는 것 대신에 리턴 값을 아웃풋 변수에 할당하고 다른 함수에 입력시켜 어떠한 경우에도 작동하는 파이프라인을 설계하게 된다. \n",
    "\n",
    "## Inner Functions \n",
    "\n",
    "### What is inner function\n",
    "\n",
    "build_csv() 함수는 Python file object를 입력받아서 CSV format으로 파일을 작성하는 함수이다. 위 함수를 작성하기 위해서는 open(file, mode)를 사용하여 file object를 젖아하기 위한 파일을 생성해야 했다. **내부 함수(inner function)**은 함수 내부에서 정의된 함수로 부모 함수의 범위(scope)내에서 캡슐화되어 사용된다. 따라서 open_file() 함수는 프로그램의 전역 범위(global scope)에서 고립되어 사용되게 된다.\n",
    "\n",
    "```Python\n",
    "def build_csv(lines, header = None, file = None) : \n",
    "    def open_file(f) : \n",
    "        if isinstance(f, str) : \n",
    "            f = open(f, 'w') \n",
    "        return f \n",
    "    \n",
    "    file = open_file(file) \n",
    "    if header : \n",
    "        lines = itertools.chain([header], lines) \n",
    "    writer = csv.writer(file, delimiter = ',') \n",
    "    writer.writerows(lines) \n",
    "    file.seek(0)\n",
    "    return file \n",
    "\n",
    "csv_file = bulid_csv(lines, file = 'exmample.csv') \n",
    "```\n",
    "\n",
    "### Inner function with partial \n",
    "\n",
    "partial() 함수는 입력받은 함수와 argument에 대해 함수를 argument를 입력하여 얼리는 효과를 발생시킨다. partial() 함수를 사용하지 않고 동일한 기능을 구현하는 함수를 작성하는 방법은 다음과 같다. \n",
    "\n",
    "```Python\n",
    "# Make new function with partial \n",
    "\n",
    "def add(a, b) : \n",
    "    return a + b\n",
    "\n",
    "add_two = partial(add, 2)\n",
    "print(add_two(7))\n",
    "\n",
    "# Make new function without partial\n",
    "\n",
    "def add(A) : \n",
    "    def inner(b) : \n",
    "        return A + b \n",
    "    return inner \n",
    "\n",
    "add_two = add(2)\n",
    "print(add_two(7))\n",
    "```\n",
    "\n",
    "새로운 add() 함수는 두가지 핵심으로 구성되어 있다.\n",
    "\n",
    "- inner() 함수는 A 변수를 add() 함수로부터 상속받아 사용한다. \n",
    "- add() 함수는 inner() 함수를 리턴한다. \n",
    "\n",
    "함수는 **일급 객체(first-class object)**로써 작동하기 때문에 inner() 함수를 하나의 값으로 간주하여 리턴할 수 있다. 즉, add() 함수에 입력된 변수 2는 A에 2를 저장한 상태로 inner()함수를 리턴하게 된다. 따라서 생성된 add_two 변수에 저장되어 있는 add(2)는 inner(b) with 2로 작업 메모리에 저장되어 있게 된다. 이러한 함수를 **closures**라고 한다. \n",
    "\n",
    "### Closures\n",
    "\n",
    "**클로저(Closure)**는 일급 객체 함수의 개념을 이용하여 스코프(scope)에 묶인 변수를 바인딩 하기 위한 일종의 기술이다. 기능상응로는 클로저는 함수를 저장한 레코드(record)이며, 스코프(scope)의 인수(factor)들은 클로저가 만들어질 때 정의되며, 스코프 내의 영역이 소멸되었어도 그에 대한 접근은 독립된 복사본인 클로저를 통해 이루어질 수 있다. \n",
    "\n",
    "즉, 클로저는 부모 함수의 변수를 조회하는 내부 함수에 의해 정의된다. 파이썬에서 부모 함수로부터 내부 함수로 전달되는 복수의 임의의 arguments들은 *를 통해 표시할 수 있다. \n",
    "\n",
    "```Python\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def partial(func, *args) : \n",
    "    parents_args = args \n",
    "    def inner(*inner_args) : \n",
    "        return func(*(parents_args + inner_args))\n",
    "    return inner\n",
    "\n",
    "add_two = partial(add, 2) \n",
    "print(add_two(7))\n",
    "```\n",
    "\n",
    "### First-Class object \n",
    "\n",
    "**일급 객체(First-class object)**란 다른 객체들에 일반적으로 적용 가능한 연산을 모드 지원하는 객체를 가리킨다. 보통 함수에 인자로 넘기기, 수정하기, 변수에 대입하기와 같은 연산을 지원할 때 일급 객체라고 한다. \n",
    "\n",
    "## Python Decorators \n",
    "\n",
    "### Debugging using Closures\n",
    "\n",
    "파이썬에서 디버깅을 목적으로 클로저를 사용해서 함수의 호출을 추적할 수 있다. \n",
    "\n",
    "```Python\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def logger(func):\n",
    "    def inner(*args):\n",
    "        print('Calling function: {}'.format(func.__name__))\n",
    "        print('With args: {}'.format(args))\n",
    "        return func(*args)\n",
    "    return inner\n",
    "\n",
    "logged_add = logger(add)\n",
    "print(logged_add(1, 2))\n",
    "```\n",
    "\n",
    "### Python Decorators \n",
    "\n",
    "매번 클로저를 사용해서 내부 함수를 호출한 뒤 함수를 한번 더 실행하는 과정은 번거로울 때가 있다. Python은 일을 더 쉽게 표현할 수 있도록 설계된 **syntatic sugar**를 사용해서 해당 과정을 간단하게 만들 수 있다. \n",
    "\n",
    "**Decorator(@)**는 주어진 상황 및 용도에 따라 어떤 객체에 책임을 덧붙이는 패턴으로, 기능 확장이 필요할 때 서브 클래싱 대신 쓸 수 있는 유연한 대안이다. 즉 사용자가 구조를 수정하지 않고 기존 객체에 새로운 기능을 추가할 수 있도록 하는 Python의 디자인 패턴이다. 데코레이터를 사용함으로써 클로저 함수에 시행하고자 하는 함수를 호출할 수 있게 되는 것이다. \n",
    "\n",
    "```Python\n",
    "def catch_error(func) : \n",
    "    def inner(*args) :\n",
    "        try : \n",
    "            return func(*args) \n",
    "        except Exception as e : \n",
    "            return e\n",
    "    return inner\n",
    "\n",
    "@catch_error \n",
    "def throws_error():\n",
    "    raise Exception('Throws Error')\n",
    "    \n",
    "print(throws_error())\n",
    "\n",
    "# We don't have to use expression below \n",
    "# catch_extract = catch_error(throws_error)\n",
    "# catch_extract() \n",
    "```\n",
    "\n",
    "### Method Decorators \n",
    "\n",
    "함수를 데코레이터로 사용하는 것 대신에 클래스 인스턴스의 메소드 또한 데코레이터로 사용할 수 있다. Pipeline 클래스는 tasks 속성과 작업을 추가하는 tasks() 메소드로 구성되어 있다. Pipeline.task()는 입력받은 작업(함수)를 클로저를 통해 추가할 수 있다. 즉 정의되는 작업은 데코레이터를 통해 추가되기만 할 뿐 Pipeline의 구조를 변화시키지는 못한다. 각각의 코드의 의미는 다음과 같다. \n",
    "\n",
    "- @pipeline.task() : 데코레이터는 클래스 메소드로부터 생성된다. \n",
    "- @pipeline.taks() : 데코레이터는 wrapping하기 전에 호출된다. \n",
    "- first_task() 함수는 리스트 객체에 다양하게 추가될 수 있다. \n",
    "\n",
    "```Python\n",
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "        \n",
    "    def task(self) : \n",
    "        def inner(func) : \n",
    "            self.tasks.append(func)\n",
    "            return func\n",
    "        return inner \n",
    "    \n",
    "pipeline = Pipeline() \n",
    "\n",
    "@pipeline.task()\n",
    "def first_task(x) :\n",
    "    return x + 1\n",
    "\n",
    "print(pipeline.tasks)\n",
    "```\n",
    "\n",
    "## Pipeline Class\n",
    "\n",
    "Pipeline 클래스는 tasks 내부에 입력값을 처리할 작업들을 task() 메소드를 통해 저장하고, run() 메소드를 통해 작업을 반복적으로 실행하게 된다. 각각의 작업들은 클로저와 데코레이터에 의해 독립적으로 정의되어 생성되고 이전의 작업에 종속된다. \n",
    "\n",
    "```Python\n",
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        idx = 0\n",
    "        if depends_on:\n",
    "            idx = self.tasks.index(depends_on) + 1\n",
    "        def inner(f):\n",
    "            self.tasks.insert(idx, f)\n",
    "            return f\n",
    "        return inner\n",
    "    \n",
    "    def run(self, input_) : \n",
    "        output = input_\n",
    "        for task in self.tasks : \n",
    "            output = task(output) \n",
    "        return output\n",
    "\n",
    "pipeline = Pipeline()\n",
    "    \n",
    "@pipeline.task()\n",
    "def first_task(x):\n",
    "    return x + 1\n",
    "\n",
    "@pipeline.task(depends_on=first_task)\n",
    "def second_task(x):\n",
    "    return x * 2\n",
    "\n",
    "@pipeline.task(depends_on=second_task)\n",
    "def last_task(x):\n",
    "    return x - 4\n",
    "\n",
    "print(pipeline.run(20))\n",
    "```\n",
    "\n",
    "## Apply Pipeline to log file \n",
    "\n",
    "기존의 open() 함수를 사용해서 빈 파일을 생성하고 값을 저장하는 방식 대신에, io module의 StringIO 객체를 사용하면 디스크 내부에 파일을 작성하지 않고 메모리 내부에 file-like 객체를 생성해서 불필요한 파일 생성을 없앨 수 있다. \n",
    "\n",
    "```Python\n",
    "# Libraries for Pipeline \n",
    "import csv \n",
    "import itertools \n",
    "import io \n",
    "\n",
    "# Tasks for Pipeline \n",
    "\n",
    "def parse_time(time_str):\n",
    "    \"\"\"\n",
    "    Parses time in the format [30/Nov/2017:11:59:54 +0000]\n",
    "    to a datetime object.\n",
    "    \"\"\"\n",
    "    time_obj = datetime.strptime(time_str, '[%d/%b/%Y:%H:%M:%S %z]')\n",
    "    return time_obj\n",
    "\n",
    "def strip_quotes(s):\n",
    "    return s.replace('\"', '')\n",
    "\n",
    "def parse_log(log):\n",
    "    for line in log:\n",
    "        split_line = line.split()\n",
    "        remote_addr = split_line[0]\n",
    "        time_local = parse_time(split_line[3] + \" \" + split_line[4])\n",
    "        request_type = strip_quotes(split_line[5])\n",
    "        request_path = split_line[6]\n",
    "        status = int(split_line[8])\n",
    "        body_bytes_sent = int(split_line[9])\n",
    "        http_referrer = strip_quotes(split_line[10])\n",
    "        http_user_agent = strip_quotes(\" \".join(split_line[11:]))\n",
    "        yield (\n",
    "            remote_addr, time_local, request_type, request_path,\n",
    "            status, body_bytes_sent, http_referrer, http_user_agent\n",
    "        )\n",
    "\n",
    "def build_csv(lines, file, header=None):\n",
    "    # if header:\n",
    "    #    lines = [header] + [l for l in lines]\n",
    "    if header:\n",
    "        lines = itertools.chain([header], lines)\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerows(lines)\n",
    "    file.seek(0)\n",
    "    return file      \n",
    "\n",
    "def count_unique_request(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('request_type')\n",
    "\n",
    "    uniques = {}\n",
    "    for line in reader:\n",
    "\n",
    "        if not uniques.get(line[idx]):\n",
    "            uniques[line[idx]] = 0\n",
    "        uniques[line[idx]] += 1\n",
    "    return ((k, v) for k, v in uniques.items())\n",
    "\n",
    "# Define Pipeline Class \n",
    "\n",
    "class Pipeline: \n",
    "    \n",
    "    def __init__(self): \n",
    "        self.tasks = [] \n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        idx = 0 \n",
    "        if depends_on : \n",
    "            idx = self.tasks.index(depends_on) + 1\n",
    "        def inner(f) : \n",
    "            self.tasks.insert(idx, f) \n",
    "            return f\n",
    "        return inner\n",
    "    \n",
    "    def run(self, input_): \n",
    "        output = input_ \n",
    "        for task in self.tasks : \n",
    "            output = task(output) \n",
    "        return output \n",
    "    \n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def parse_logs(logs):\n",
    "    return parse_log(logs)\n",
    "\n",
    "@pipeline.task(depends_on=parse_logs)\n",
    "def build_raw_csv(lines):\n",
    "    return build_csv(lines, header=[\n",
    "        'ip', 'time_local', 'request_type',\n",
    "        'request_path', 'status', 'bytes_sent',\n",
    "        'http_referrer', 'http_user_agent'\n",
    "    ],\n",
    "    file=io.StringIO())\n",
    "\n",
    "@pipeline.task(depends_on=build_raw_csv)\n",
    "def count_uniques(csv_file):\n",
    "    return count_unique_request(csv_file)\n",
    "\n",
    "@pipeline.task(depends_on=count_uniques)\n",
    "def summarize_csv(lines):\n",
    "    return build_csv(lines, header=['request_type', 'count'], file=io.StringIO())\n",
    "\n",
    "log = open('example_log.txt')\n",
    "summarized_file = pipeline.run(log)\n",
    "print(summarized_file.readlines())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4bc93b",
   "metadata": {},
   "source": [
    "# Multiple Dependency Pipeline\n",
    "\n",
    "## What is DAG\n",
    "\n",
    "**유향 비순환 그래프(Direct acyclic graph;DAG)**, 방향 비순환 그래프는 개별 요소들이 특정한 방향을 향하고 있으며, 서로 순환하지 않는 구조로 짜여진 그래프를 말한다. 직역 그대로 한 방향으로 이어지지만, 순환은 하지 않는 그래프이다. \n",
    "\n",
    "- Directed : Each edge of a vertex poins only in one direction\n",
    "- Acyclic : The graph does not have any cycle \n",
    "- Graph : The data structure is composed of verticles(nodes) and edges(branches) \n",
    "\n",
    "이전에 생성했던 Pipeline 클래스는 하나의 선형 작업으로 이루어진 작업만 가능하도록 제한되었다. 선형 순서 대신에 부양하는 가지를 가지고 있는 파이프라인을 생성할 경우 DAG가 사용된다. DAG구조는 자연스럽게 부양하는 작업의 효율적인 순서를 생성한다. 대표적으로 작업 스케쥴링을 하는데 좋은 알고리즘으로 사용된다. \n",
    "\n",
    "## The DAG Class\n",
    "\n",
    "DAG 클래스 내부에 부양하는 작업을 가지고 있는 그래프를 생성하기 위해 vertices 리스트를 가지고 있는 Vertex 클래스를 정의한다. 이후 트리 구조의 루트 노드와 같이, DAG클래스는 Vertex 클래스로 구성된다. 하지만 이 구조는 단일 루트로 시작하는 그래프 구조만 설명할 수 있다. \n",
    "\n",
    "```Python\n",
    "class DAG:\n",
    "    def __init__(self):\n",
    "        self.root = Vertex() \n",
    "        \n",
    "class Vertex: \n",
    "    def __init__(self):\n",
    "        self.to = [] \n",
    "        self.data = None\n",
    "```\n",
    "\n",
    "복수의 루트 노드를 갖는 DAG 구조를 생성하기 위해 리스트 값을 가지는 딕셔너리를 그래프 구조로 가지는 DAG 클래스를 생성한다. graph 속성의 key는 구조 내부에 존재하는 모든 노드이고, value는 key 노드가 진행할 수 있는 모든 노드를 저장한 리스트이다. \n",
    "\n",
    "```Python\n",
    "class DAG(DQ)\n",
    "    def __init__(self):\n",
    "        self.graph = {}\n",
    "    \n",
    "    def add(self, node, to = None): \n",
    "        if not node in self.graph:\n",
    "            self.graph[node] = [] \n",
    "        if to: \n",
    "            self.graph[node].append(to) \n",
    "            self.graph[to] = [] \n",
    "```\n",
    "\n",
    "## Sorting the DAG\n",
    "\n",
    "기존의 정렬 알고리즘은 내림차순이나 오름차순으로 값을 정렬하는 작업을 수행한다. DAG의 정렬 알고리즘은 부양하는 가지를 중심으로 정렬하게 된다. 정렬하는 기준은 부양하는 작업이 가장 많은 작업부터 시작해서 부양하는 작업이 가장 없는 작업으로 진행된다. 이전의 파이프라인의 경우에는 파일을 파싱하는 것부터 시작해서 요약으로 종료된다. \n",
    "\n",
    "DAG 구조에서 노드의 경로가 길어지면 길어질수록 부양하는 노드의 수가 줄어들게 된다. 이 가설을 토대로 워크 플로우를 생성하면 다음과 같다.\n",
    "\n",
    "1. 진행한 경로가 0인 루트 노드를 찾는다.\n",
    "2. 각각의 노드에 대해서 루트노드부터 가장 긴 경로를 찾는다.\n",
    "3. 가장 긴 경로에 대해서 정렬한다. \n",
    "\n",
    "## Finding Number of In Degrees \n",
    "\n",
    "가장 긴 경로를 찾기 위해, 지시한 방향을 어떤 노드가 시작하는지 알아야 한다. 시작하는 노드는 그래프가 확장하는 루트 노드를 의미한다. 루트 노드를 찾기 위해 in-degree 메트릭을 사용한다. in-degrees는 노드로 향하는 화살표(edges pointing)의 전체 수이다. 이 수가 0이되는 노드가 루트 노드가 된다. \n",
    "\n",
    "```Python\n",
    "class DAG: \n",
    "    def __init__(self):\n",
    "        self.graph = {}\n",
    "    \n",
    "    def add(self, node, to = None): \n",
    "        if not node in self.graph:\n",
    "            self.graph[node] = [] \n",
    "        if to: \n",
    "            self.graph[node].append(to) \n",
    "            self.graph[to] = [] \n",
    "            \n",
    "    def in_degrees(self):\n",
    "        self.degrees = {} \n",
    "        for node in self.graph: \n",
    "            if node not in self.degrees: \n",
    "                self.degrees[node] = 0\n",
    "            for point in self.graph[node]: \n",
    "                if point not in self.degrees: \n",
    "                    self.degrees[point] = 0\n",
    "                self.degrees[point] += 1\n",
    "```\n",
    "\n",
    "## Walking in Degrees\n",
    "\n",
    "in_degrees 메소드를 통해 생성된 self.degrees는 어떤 노드가 루트 노드인지 확인할 수 있다. self.degrees의 value가 0이 되는 노드가 루트 노드가 된다. 이전에 설정했던 가정은 다음과 같다. 경로가 짧으면 짧을수록 노드의 의존성(중요도)는 증가하게 된다. 따라서 중요도에 따라 노드를 정렬하기 위해 다음 작업을 진행해야 한다. \n",
    "\n",
    "1. 모든 루트 노드를 그래프 구조에서 pop하여 큐에 추가한다. \n",
    "2. 각각의 루트 노드의 pointers를 확인하고 새로운 루트가 되는지 확인한다.\n",
    "    1. 루트노드라면, 루트 노드 큐에 붙이고 그래프 구조에서 pop한다. \n",
    "    2. 아니라면 계속 진행한다. \n",
    "3. 그래프 구조에서 pop된 노드에 대해서 정렬한다. \n",
    "\n",
    "다음 알고리즘에 의해 노드의 순서는 '중요도'에 따라 정렬된다. 알고리즘을 구현하기 위해 queue 자료구조를 사용한다. \n",
    "\n",
    "1. in_degrees에서 루트 노드를 queue에 위치시킨다.\n",
    "2. queue가 비어있을 때까지 아래 작업을 반복한다. \n",
    "    1. node_i를 dequeue한다.\n",
    "    2. node_i의 pointers를 확인한다.\n",
    "    3. node_i의 모든 pointer의 in_degree를 1만큼 감소시킨다. \n",
    "    4. 만일 in_degree의 값이 0이라면, queue에 추가한다.\n",
    "    5. 0이 아니라면 큐에 존재하는 새로운 루트 노드에 대해서 재귀적으로 진행한다. \n",
    "    6. 모든 pointers가 탐색되면 node_i를 searched에 추가한다.\n",
    "3. searched 리스트를 반환한다. \n",
    "\n",
    "```Python\n",
    "from collections import deque\n",
    "\n",
    "class DAG: \n",
    "    def __init__(self):\n",
    "        self.graph = {}\n",
    "    \n",
    "    def add(self, node, to = None): \n",
    "        if not node in self.graph:\n",
    "            self.graph[node] = [] \n",
    "        if to: \n",
    "            self.graph[node].append(to) \n",
    "            self.graph[to] = [] \n",
    "            \n",
    "    def in_degrees(self):\n",
    "        self.degrees = {} \n",
    "        for node in self.graph: \n",
    "            if node not in self.degrees: \n",
    "                self.degrees[node] = 0\n",
    "            for point in self.graph[node]: \n",
    "                if point not in self.degrees: \n",
    "                    self.degrees[point] = 0\n",
    "                self.degrees[point] += 1\n",
    "                \n",
    "    def sort(self): \n",
    "        self.in_degrees()\n",
    "        visit = queue() \n",
    "        \n",
    "        for node in self.graph: \n",
    "            if self.degrees[node] == 0:\n",
    "                visit.append(node) \n",
    "                \n",
    "        searched = []\n",
    "        while visit:\n",
    "            node = visit.popleft() \n",
    "            for pointer in self.graph[node]: \n",
    "                self.degrees[pointer] -= 1\n",
    "                if self.degrees[pointer] == 0: \n",
    "                    visit.append(pointer) \n",
    "            searched.append(node) \n",
    "        return searched\n",
    "```\n",
    "\n",
    "## Enhanced the Add Method \n",
    "\n",
    "이전에 작성했던 알고리즘이 DAG 정렬 알고리즘의 **Kahn's Algorithm**이다. 이 정렬 알고리즘의 속성은 그래프의 구조가 순환인지 아닌지를 결정할 수 있다. 이를 확인하는 방법은 정렬된 노드의 길이가 그래프의 노드의 길이와 일치하지 않으면 순환하는 지점이 있는 것을 의미하고 에러를 출력한다. \n",
    "\n",
    "```Python\n",
    "from collections import deque\n",
    "\n",
    "class DAG: \n",
    "    def __init__(self):\n",
    "        self.graph = {}\n",
    "    \n",
    "    def add(self, node, to = None): \n",
    "        if not node in self.graph:\n",
    "            self.graph[node] = [] \n",
    "        if to: \n",
    "            self.graph[node].append(to) \n",
    "            self.graph[to] = [] \n",
    "            \n",
    "        if len(self.sort()) != len(self.graph):\n",
    "            raise Exception \n",
    "            \n",
    "    def in_degrees(self):\n",
    "        self.degrees = {} \n",
    "        for node in self.graph: \n",
    "            if node not in self.degrees: \n",
    "                self.degrees[node] = 0\n",
    "            for point in self.graph[node]: \n",
    "                if point not in self.degrees: \n",
    "                    self.degrees[point] = 0\n",
    "                self.degrees[point] += 1\n",
    "                \n",
    "    def sort(self): \n",
    "        self.in_degrees()\n",
    "        visit = queue() \n",
    "        \n",
    "        for node in self.graph: \n",
    "            if self.degrees[node] == 0:\n",
    "                visit.append(node) \n",
    "                \n",
    "        searched = []\n",
    "        while visit:\n",
    "            node = visit.popleft() \n",
    "            for pointer in self.graph[node]: \n",
    "                self.degrees[pointer] -= 1\n",
    "                if self.degrees[pointer] == 0: \n",
    "                    visit.append(pointer) \n",
    "            searched.append(node) \n",
    "        return searched\n",
    "```\n",
    "\n",
    "## Adding DAG to the Pipeline\n",
    "\n",
    "생성한 DAG 클래스를 Pipeline 객체에 적용시키게 되면, 기존의 self.tasks = []를 DAG()로 수정하면 된다. 이후 작업을 추가할 때, depends_on 함수가 없으면 self.tasks에 더하고, depends_on이 존재하면 dependency를 추가해서 tasks를 업데이트한다. \n",
    "\n",
    "```Python\n",
    "class Pipeline(DQ):\n",
    "    def __init__(self):\n",
    "        self.tasks = DAG()\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        def inner(f):\n",
    "            self.tasks.add(f)\n",
    "            if depends_on:\n",
    "                self.tasks.add(depends_on, f)\n",
    "            return f\n",
    "        return inner\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def first():\n",
    "    return 20\n",
    "\n",
    "@pipeline.task(depends_on = first) \n",
    "def second(x):\n",
    "    return x * 2\n",
    "\n",
    "@pipeline.task(depends_on = second)\n",
    "def third(x):\n",
    "    return x // 3\n",
    "\n",
    "@pipeline.task(depends_on = second)\n",
    "def fourth(x):\n",
    "    return x // 4\n",
    "\n",
    "graph = pipeline.tasks.graph\n",
    "```\n",
    "\n",
    "## Run the Pipeline \n",
    "\n",
    "위 task에 추가된 함수에는 입력값이 없다. 따라서 정적 객체를 반환하는 파이프라인 실행 메소드를 작성해야 한다. 또한 DAG구조에서 마지막 작업의 개념은 존재하지 않기 때문에, 작업의 결과를 나타내기 위해서 output이 실행되는 결과를 function : output으로 매핑시켜 결과를 저장해야 한다. \n",
    "\n",
    "1. task.sort()를 시행하여 정렬된 task를 visited에 저장한다. \n",
    "2. function : output으로 매핑되는 결과를 저장하기 위한 competed를 초기화한다. \n",
    "3. visited 내의 모든 task를 순환한다.\n",
    "    1. 현재 task에 참조되어 있는 모든 node(이전 task)와 values(현재 tasks)를 조회하여 작업이 존재할 경우, 이전 task의 시행 결과를 현재 task의 입력값으로 넣어 completed에 저장한다. \n",
    "    2. 참조되는 작업이 없을 경우 함수의 시행 결과를 completed에 저장한다. \n",
    "    \n",
    "```Python\n",
    "class Pipeline(DQ):\n",
    "    def __init__(self):\n",
    "        self.tasks = DAG()\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        def inner(f):\n",
    "            self.tasks.add(f)\n",
    "            if depends_on:\n",
    "                self.tasks.add(depends_on, f)\n",
    "            return f\n",
    "        return inner\n",
    "    \n",
    "    def run(self): \n",
    "        visited = self.tasks.sort()\n",
    "        completed = {} \n",
    "        for task in visited : \n",
    "            for node, values in self.tasks.graph.items():\n",
    "                if task in values : \n",
    "                    completed[task] = task(completed[node])\n",
    "            if task not in completed : \n",
    "                completed[task] = task() \n",
    "        return completed\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def first():\n",
    "    return 20\n",
    "\n",
    "@pipeline.task(depends_on = first) \n",
    "def second(x):\n",
    "    return x * 2\n",
    "\n",
    "@pipeline.task(depends_on = second)\n",
    "def third(x):\n",
    "    return x // 3\n",
    "\n",
    "@pipeline.task(depends_on = second)\n",
    "def fourth(x):\n",
    "    return x // 4\n",
    "\n",
    "outputs = pipeline.run()\n",
    "print(outputs)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
